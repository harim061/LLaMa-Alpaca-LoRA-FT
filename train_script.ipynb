{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969d3f69e3ae4e1abe2711f1dac20538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"gpu available\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Compute device is:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26df390a26654c029b9fef86da032beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"baffo32/decapoda-research-llama-7B-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 요아정 왔다!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Alpaca-LoRA model with params:\n",
      "base_model: baffo32/decapoda-research-llama-7B-hf\n",
      "data_path: sharegpt_deepl_ko\\ko_alpaca_style_dataset.json\n",
      "output_dir: ./output\n",
      "batch_size: 64\n",
      "micro_batch_size: 8\n",
      "num_epochs: 5\n",
      "learning_rate: 0.0005\n",
      "cutoff_len: 256\n",
      "val_set_size: 2000\n",
      "lora_r: 8\n",
      "lora_alpha: 16\n",
      "lora_dropout: 0.05\n",
      "lora_target_modules: ['q_proj', 'v_proj']\n",
      "train_on_inputs: True\n",
      "add_eos_token: False\n",
      "group_by_length: False\n",
      "wandb_project: \n",
      "wandb_run_name: \n",
      "wandb_watch: \n",
      "wandb_log_model: \n",
      "resume_from_checkpoint: False\n",
      "prompt template: custom\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\alpaca-lora\\finetune.py\", line 283, in <module>\n",
      "    fire.Fire(train)\n",
      "  File \"c:\\Users\\DS\\anaconda3\\envs\\autotrain\\lib\\site-packages\\fire\\core.py\", line 143, in Fire\n",
      "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
      "  File \"c:\\Users\\DS\\anaconda3\\envs\\autotrain\\lib\\site-packages\\fire\\core.py\", line 477, in _Fire\n",
      "    component, remaining_args = _CallAndUpdateTrace(\n",
      "  File \"c:\\Users\\DS\\anaconda3\\envs\\autotrain\\lib\\site-packages\\fire\\core.py\", line 693, in _CallAndUpdateTrace\n",
      "    component = fn(*varargs, **kwargs)\n",
      "  File \"c:\\alpaca-lora\\finetune.py\", line 112, in train\n",
      "    model = LlamaForCausalLM.from_pretrained(\n",
      "  File \"c:\\Users\\DS\\anaconda3\\envs\\autotrain\\lib\\site-packages\\transformers\\modeling_utils.py\", line 3452, in from_pretrained\n",
      "    hf_quantizer.validate_environment(device_map=device_map)\n",
      "  File \"c:\\Users\\DS\\anaconda3\\envs\\autotrain\\lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_8bit.py\", line 86, in validate_environment\n",
      "    raise ValueError(\n",
      "ValueError: \n",
      "                    Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the\n",
      "                    quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules\n",
      "                    in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to\n",
      "                    `from_pretrained`. Check\n",
      "                    https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n",
      "                    for more details.\n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "!python finetune.py \\\n",
    "    --base_model 'baffo32/decapoda-research-llama-7B-hf' \\\n",
    "    --data_path 'sharegpt_deepl_ko\\ko_alpaca_style_dataset.json' \\\n",
    "    --output_dir './output' \\\n",
    "    --num_epochs 5 \\\n",
    "    --learning_rate 5e-4 \\\n",
    "    --val_set_size 2000 \\\n",
    "    --batch_size 64 \\\n",
    "    --micro_batch_size 8 \\\n",
    "    --prompt_template_name 'custom'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 6.442123264 GB\n",
      "Used GPU Memory: 4.343382016 GB\n",
      "Remaining GPU Memory: 2.098741248 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 현재 사용 중인 GPU 메모리\n",
    "torch.cuda.memory_allocated()\n",
    "\n",
    "# 전체 GPU 메모리\n",
    "torch.cuda.max_memory_allocated()\n",
    "\n",
    "# 남은 GPU 메모리 계산\n",
    "total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "used_memory = torch.cuda.memory_allocated()\n",
    "remaining_memory = total_memory - used_memory\n",
    "\n",
    "print(f\"Total GPU Memory: {total_memory / 1e9} GB\")\n",
    "print(f\"Used GPU Memory: {used_memory / 1e9} GB\")\n",
    "print(f\"Remaining GPU Memory: {remaining_memory / 1e9} GB\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autotrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
